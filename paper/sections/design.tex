\section{Design}
\label{sec:design}

Evidence provides a declarative API for specifying correctness
properties of Python functions. This section describes the contract
language, strategy synthesis, testing protocol, and the structured
output that enables agent integration.

\subsection{Contract Language}
\label{sec:contracts}

Evidence defines five decorators that attach machine-checkable
contracts to Python functions. A developer writes the specification
and contracts; the agent writes (or modifies) the implementation.
Decorators stack in bottom-up order: innermost decorators (closest to
the function definition) are applied first.

\paragraph{\dmark{spec}.}
The \dmark{spec} decorator marks a function as a reference
specification---a trusted, obviously-correct implementation that may be
too slow or too simple for production use. Specification functions
serve as oracles for equivalence testing. Because specifications are
typically short and straightforward, they are easy for a human to
audit---a critical property when the implementation is generated by an
agent.

\paragraph{\dmark{against(spec\_fn)}.}
The \dmark{against} decorator declares that the decorated function
should produce the same output as \texttt{spec\_fn} for all inputs
satisfying the function's preconditions. It accepts four optional
parameters: \texttt{max\_examples} (default 200) controls the number
of randomized test cases, \texttt{deadline\_ms} sets a per-example
time limit, \texttt{suppress\_health\_checks} configures Hypothesis
health check suppression, and \texttt{eq} specifies a custom equality
function for comparing outputs. The \texttt{eq} parameter accepts
either a callable or the string \texttt{"approx"}, which dispatches to
\texttt{math.isclose}, \texttt{numpy.allclose}, or
\texttt{torch.allclose} depending on the output type.

\paragraph{\dmark{requires(pred)}.}
The \dmark{requires} decorator attaches a precondition. The predicate
\texttt{pred} takes the same arguments as the decorated function and
returns a boolean. During testing, Evidence filters generated inputs
that violate preconditions via Hypothesis's \texttt{assume()}
mechanism, ensuring that only valid inputs reach the function under
test. Multiple \dmark{requires} decorators stack: all predicates must
hold.

\paragraph{\dmark{ensures(pred)}.}
The \dmark{ensures} decorator attaches a postcondition. The predicate
receives the function's original arguments plus the return value
(as a keyword argument \texttt{result}) and returns a boolean.
Evidence checks postconditions after every function invocation during
testing. Multiple \dmark{ensures} decorators stack.

\paragraph{\dmark{pure}.}
The \dmark{pure} decorator asserts that a function has no side effects
and is deterministic. Evidence verifies this assertion through both
static analysis (AST inspection for IO operations, nondeterminism, and
global mutation) and dynamic analysis (calling the function twice with
identical inputs and asserting output equality). An optional
\texttt{seed} parameter enables \emph{seed-deterministic} mode, where
the function may use pseudorandom number generators internally but
must produce identical outputs when all PRNGs are seeded to the same
value. In seed-deterministic mode, Evidence seeds Python's
\texttt{random}, NumPy's \texttt{numpy.random}, and PyTorch's
\texttt{torch.manual\_seed} before each invocation.

\subsection{Strategy Synthesis}
\label{sec:strategies}

Evidence automatically synthesizes Hypothesis strategies from Python
type annotations, eliminating the need to write generators manually.
This is particularly important for agent workflows, where requiring
the agent to write correct Hypothesis strategies would introduce an
additional source of error. The synthesis algorithm handles the
following types:

\begin{itemize}
\item \textbf{Primitives:} \texttt{int}, \texttt{float},
  \texttt{bool}, \texttt{str}, \texttt{bytes}.
\item \textbf{Generic collections:} \texttt{list[T]},
  \texttt{dict[K,V]}, \texttt{set[T]}, \texttt{tuple[T, ...]},
  fixed-length tuples.
\item \textbf{Unions:} \texttt{Optional[T]} and
  \texttt{Union[T1, T2, ...]}.
\item \textbf{Dataclasses:} Evidence recursively generates strategies
  for each field, with depth limiting (maximum depth~5) to prevent
  infinite recursion on recursive types.
\item \textbf{Custom types:} Developers register strategies for
  domain-specific types via \texttt{register\_strategy(type, strat)}
  or parameterized factories via
  \texttt{register\_strategy\_factory(type, factory)}.
\end{itemize}

For function-level strategy composition, Evidence inspects the
function signature via \texttt{inspect.signature} and
\texttt{get\_type\_hints}, builds a strategy for each parameter, and
combines them into a single \texttt{fixed\_dictionaries} strategy that
produces keyword-argument dictionaries.

\subsection{Two-Phase Testing Protocol}
\label{sec:protocol}

For each annotated function, Evidence executes two phases:

\paragraph{Phase 1: Smoke test.}
Evidence generates a single input satisfying all preconditions, calls
the function, and checks all postconditions. This phase uses a reduced
collection size (default 5 elements) for fast execution.
Evidence finds a satisfying input by calling
\texttt{hypothesis.find} with a predicate that checks all
\dmark{requires} predicates. If no satisfying input exists (the
preconditions are unsatisfiable), Evidence reports a
\texttt{requires\_satisfiable} failure and skips subsequent phases.

\paragraph{Phase 2: Specification equivalence.}
If the function has an \dmark{against} annotation, Evidence searches
for an input where the implementation and specification disagree. This
phase proceeds in two steps:

\begin{enumerate}
\item \emph{Deterministic probe.} Evidence calls
  \texttt{hypothesis.find} with a predicate that returns \texttt{True}
  when the implementation's output differs from the specification's
  output (or when a postcondition fails). If \texttt{find} locates a
  counterexample, Evidence reports it immediately.

\item \emph{Randomized search.} If the deterministic probe finds no
  counterexample, Evidence runs a \texttt{@given}-based property test
  with the full \texttt{max\_examples} budget. This search uses
  Hypothesis's shrinking to produce a minimal counterexample if one
  exists.
\end{enumerate}

The two-step approach balances speed and thoroughness: the
deterministic probe catches obvious discrepancies quickly, while the
randomized search provides higher confidence when no counterexample is
found.

\subsection{Structured Output for Agent Integration}
\label{sec:output}

Evidence produces output in formats designed for both human and agent
consumption. The default CLI mode prints one colored line per
obligation with status (\textsc{pass}, \textsc{fail}, \textsc{error},
or \textsc{skip}) and timing. JSON mode (\texttt{-{}-json}) writes a
machine-readable array to standard output---the primary interface for
agent integration.

Each result is represented as an \texttt{ObligationResult} with five
fields: \texttt{function} (qualified name), \texttt{obligation} (check
type), \texttt{status}, \texttt{details} (a dictionary containing
counterexamples, implementation output, and specification output), and
\texttt{duration\_s} (wall-clock time). An agent that receives a
\texttt{fail} result can extract the counterexample from
\texttt{details}, compare \texttt{impl\_result} with
\texttt{spec\_result}, identify the discrepancy, and attempt a
targeted repair.

Evidence also writes two JSON files per module to an output directory
(default \texttt{.evidence/}): an obligations file containing
per-obligation results with status, details, and duration, and a trust
file containing module metadata and a list of checked functions.

The programmatic API (\texttt{check\_module}) provides the same
structured results via Python objects, with an optional
\texttt{on\_result} callback that fires after each obligation is
checked. An agent can call \texttt{check\_module} directly, inspect
results programmatically, and feed counterexample data into its repair
logic without parsing CLI output.
