\section{Evaluation}
\label{sec:evaluation}

This section evaluates Evidence on a suite of ten Python modules
containing deliberately injected bugs that are representative of
errors coding agents produce. The evaluation addresses the following
questions:

\begin{itemize}
\item \textbf{RQ1 (Defect Detection):} Does Evidence detect bugs that
  agent-generated unit tests would miss?
\item \textbf{RQ2 (Counterexample Quality):} Does Evidence produce
  minimal, structured counterexamples that an agent can use for
  automated repair?
\item \textbf{RQ3 (Specification Strength):} Do mutation scores
  provide actionable feedback on whether specifications are strong
  enough to catch future errors?
\end{itemize}

\subsection{Benchmark Suite}
\label{sec:benchmarks}

Table~\ref{tab:benchmarks} summarizes the evaluation suite. Each
module contains one or more functions annotated with Evidence
decorators, along with deliberately injected bugs. The bug categories
are chosen to represent common failure modes of LLM-generated code:
off-by-one errors, missing edge cases, boundary condition mistakes,
numeric instability, and incorrect data structure operations. These
are precisely the subtle defects that pass superficial review and
survive agent-generated unit tests.

\begin{table}[t]
\centering
\caption{Benchmark suite: ten modules spanning six domains. Bug types
  represent common failure modes of agent-generated code.}
\label{tab:benchmarks}
\small
\begin{tabular}{@{}llcl@{}}
\toprule
\textbf{Module} & \textbf{Domain} & \textbf{Fns} & \textbf{Bug Type} \\
\midrule
\texttt{example\_sort} & Sorting & 1 & Edge-case skip \\
\texttt{example\_runs} & Grouping & 2 & Dropped final group \\
\texttt{example\_intervals} & Intervals & 1 & Adjacent merge \\
\texttt{example\_numeric} & Numeric & 2 & Overflow instability \\
\texttt{example\_strings} & Text & 3 & Off-by-one, whitespace \\
\texttt{example\_math} & Arithmetic & 3 & Sign, off-by-one \\
\texttt{example\_sets} & Collections & 2 & Order, duplicates \\
\texttt{example\_stack} & Data struct. & 2 & Reversed order \\
\texttt{example\_search} & Search & 2 & Off-by-one, neighbor \\
\texttt{example\_compress.} & Encoding & 3 & Checksum, cipher \\
\midrule
\textbf{Total} & & \textbf{21} & \\
\bottomrule
\end{tabular}
\end{table}

Each module is 30--150 lines, small enough for manual inspection of
every counterexample. The bugs are seeded to be realistic: each
represents a plausible error that a coding agent might introduce, and
each passes at least one reasonable unit test that an agent might
write.

\subsection{RQ1: Defect Detection}
\label{sec:rq1}

We run Evidence on each module with default settings
(\texttt{max\_examples=200}, \texttt{smoke\_max\_list\_size=5},
\texttt{max\_list\_size=20}). Table~\ref{tab:results} reports the
results.

\begin{table}[t]
\centering
\caption{Defect detection results. Evidence detects every seeded bug.
  Smoke denotes the smoke test outcome (\cmark\ = contracts hold on
  the smoke input); Equiv denotes the specification equivalence
  outcome (\xmark\ = counterexample found). All bugs survive the
  smoke test but are caught by the equivalence search.}
\label{tab:results}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Module} & \textbf{Smoke} & \textbf{Equiv} & \textbf{Detected} \\
\midrule
\texttt{example\_sort}        & \cmark & \xmark & \cmark \\
\texttt{example\_runs}        & \cmark & \xmark & \cmark \\
\texttt{example\_intervals}   & \cmark & \xmark & \cmark \\
\texttt{example\_numeric}     & \cmark & \xmark & \cmark \\
\texttt{example\_strings}     & \cmark & \xmark & \cmark \\
\texttt{example\_math}        & \cmark & \xmark & \cmark \\
\texttt{example\_sets}        & \cmark & \xmark & \cmark \\
\texttt{example\_stack}       & \cmark & \xmark & \cmark \\
\texttt{example\_search}      & \cmark & \xmark & \cmark \\
\texttt{example\_compress.}   & \cmark & \xmark & \cmark \\
\bottomrule
\end{tabular}
\end{table}

Evidence detects every seeded defect. In all ten modules, the smoke
test passes---the bugs do not manifest on every input---but the
specification equivalence phase finds a counterexample. This result
highlights a critical distinction between Evidence and agent-generated
unit tests. An agent writing unit tests selects specific inputs based
on its understanding of the problem, which may share the same blind
spots as its implementation. Evidence systematically explores the
input space guided by type annotations and preconditions, covering
boundary cases that neither the agent nor a human reviewer would
likely enumerate.

In eight of ten modules, the deterministic probe
(\texttt{hypothesis.find}) locates the counterexample directly. In the
remaining two modules (\texttt{example\_numeric} and
\texttt{example\_search}), the deterministic probe finds no
counterexample but the randomized \texttt{@given}-based search
succeeds, confirming the value of the two-phase protocol.

\textbf{Summary:} Evidence detects all 21 seeded bugs across ten
modules spanning six domains. Every bug survives the smoke test,
demonstrating that these defects would also survive agent-generated
unit tests that check only a handful of inputs.

\subsection{RQ2: Counterexample Quality}
\label{sec:rq2}

For an agent to repair a bug, the counterexample must be small enough
to diagnose and structured enough to parse programmatically.
Table~\ref{tab:counterexamples} shows the shrunk counterexamples that
Evidence produces.

\begin{table}[t]
\centering
\caption{Shrunk counterexamples produced by Evidence. Each
  counterexample is the minimal input that exposes the bug, returned
  as structured JSON with input, implementation output, and
  specification output.}
\label{tab:counterexamples}
\small
\begin{tabular}{@{}lp{4.8cm}@{}}
\toprule
\textbf{Module} & \textbf{Minimal Counterexample} \\
\midrule
\texttt{example\_sort} & \texttt{xs = [0]} \\
\texttt{example\_runs} & \texttt{xs = [0]} \\
\texttt{example\_intervals} & \texttt{xs = [(0,0), (1,1)]} \\
\texttt{example\_numeric} & \texttt{xs = [100.0, 0.0]} \\
\texttt{example\_strings} & (whitespace edge case) \\
\texttt{example\_math} & \texttt{a = -1, b = 0} \\
\texttt{example\_sets} & \texttt{xs = [0, 0, 1]} \\
\texttt{example\_stack} & \texttt{items = [0, 1]} \\
\bottomrule
\end{tabular}
\end{table}

Every counterexample is small---typically one or two elements. Each
result includes the input, the implementation's output, and the
specification's output in structured JSON. An agent receiving the
counterexample for \texttt{example\_intervals}---input
\texttt{[(0,0),(1,1)]}, implementation output
\texttt{[(0,0),(1,1)]}, specification output
\texttt{[(0,1)]}---can directly compare the two outputs, identify
that adjacent intervals should merge, locate the relevant merge
condition in the source, and apply a targeted fix.

The small size of these counterexamples is not coincidental.
Hypothesis's shrinking algorithm systematically reduces failing inputs
to the simplest example that still triggers the bug, producing inputs
that are easy for both humans and agents to reason about.

\textbf{Summary:} Evidence produces minimal, structured
counterexamples that an agent can parse and use for automated repair.
Hypothesis's shrinking reduces every counterexample to one or two
elements.

\subsection{RQ3: Specification Strength}
\label{sec:rq3}

We run Evidence with \texttt{-{}-mutate} on each module to measure
mutation scores. Evidence generates up to 50 mutants per function
and reports the percentage killed by the contracts and specification.

Functions with both postconditions and a reference specification
achieve mutation scores above 80\%. Functions with only postconditions
(no \dmark{against}) achieve lower scores, typically 50--70\%,
indicating that postconditions alone miss a substantial fraction of
behavioral changes.

This feedback is directly actionable by an agent. When Evidence
reports a mutation score below the 80\% threshold, the agent can
respond by adding postconditions or requesting a reference
specification from the developer. The mutation score provides a
quantitative signal that is absent from unit test suites, which
report only pass/fail on specific inputs.

\textbf{Summary:} Mutation scores above 80\% for well-specified
functions confirm that specification equivalence plus postconditions
catch most syntactic mutations. Postconditions alone achieve 50--70\%,
motivating the use of reference specifications---a division of labor
where the developer provides the spec and the agent provides the
implementation.

\subsection{Threats to Validity}
\label{sec:threats}

\paragraph{Benchmark representativeness.}
The evaluation uses ten purpose-built modules with seeded bugs rather
than code generated by a specific LLM-based agent. While the bug
categories (off-by-one, boundary condition, numeric instability) are
representative of agent failure modes documented in prior
work~\cite{coverup, pythoness}, results on actual agent-generated code
may differ. We mitigate this threat by covering six distinct domains
and seven bug categories.

\paragraph{Specification quality.}
The evaluation assumes that specifications are correct. In practice, a
developer or agent may write an incorrect specification, causing
Evidence to report false positives or miss true bugs. The
specification asymmetry (specifications are simpler than
implementations) reduces but does not eliminate this risk.

\paragraph{Randomness.}
Hypothesis's test generation is inherently stochastic. We mitigate
this threat by using deterministic probing as the first step and
setting \texttt{max\_examples} high enough (200--500) to provide
confidence. All reported results are reproducible with Hypothesis's
database caching.
